{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87175946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d2d6e9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
      "32033\n"
     ]
    }
   ],
   "source": [
    "# If the names have not been downloaded\n",
    "if not os.path.exists('names.txt'):\n",
    "    # download the names.txt file from github\n",
    "    !wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
    "\n",
    "with open('names.txt','r') as h:\n",
    "    words = [w.rstrip() for w in h.readlines()]\n",
    "\n",
    "print(words[:8])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b1116b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total characters: N=27\n",
      "\n",
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "1\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(''.join(words)))\n",
    "chars = ['.']+chars\n",
    "\n",
    "#\n",
    "# same result as two rows above as '.' is less than any [a-z] or [0-9]\n",
    "# chars = sorted(set('.'.join(words))):\n",
    "# print('a' > '.')\n",
    "# (True)\n",
    "\n",
    "# same:\n",
    "#chars = ['.']+[*chars]\n",
    "\n",
    "N = len(chars)\n",
    "print(f\"\\nTotal characters: {N=}\\n\")\n",
    "\n",
    "stoi = { s:i for i,s in enumerate(chars)}\n",
    "itos = { i:s for s,i in stoi.items()}\n",
    "print(itos)\n",
    "\n",
    "encode = lambda c : stoi[c]\n",
    "decode = lambda i : itos[i]\n",
    "\n",
    "print(encode('a'))\n",
    "print(decode(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2183a933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b0f0c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    \n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=g)/fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "#       __init__() must return None\n",
    "#         return 2 <-- will cause an error\n",
    "#       __call__() can return a value, still __call__() seems like an odd shortcut\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        # NOTE usage of () here:\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "    \n",
    "class BatchNorm1d:\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        \n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        \n",
    "        # buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        if self.training:\n",
    "            xmean = x.mean(dim=0, keepdim=True)\n",
    "            xvar  = x.var(dim=0, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "            \n",
    "        xhat = (x - xmean)/torch.sqrt(xvar+self.eps)\n",
    "        \n",
    "        #\n",
    "        # a typo here (had self.beta*xhat+self.beta)\n",
    "        # was causing:\n",
    "        #\n",
    "        # the failure below \n",
    "        #\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum) * self.running_mean +  self.momentum * xmean\n",
    "                self.running_var = (1-self.momentum) * self.running_var +  self.momentum * xvar\n",
    "                \n",
    "        return self.out # this is not standard    \n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "                    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2247aa7e",
   "metadata": {},
   "source": [
    "--> 35     p.data -= lr*p.grad\n",
    "     37 if step % int(Steps/1000) == 0:\n",
    "     38     print(f\"{step = },\\tloss = {loss.item()}\")\n",
    "\n",
    "TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f7c36422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47024\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# model initialization again, but with batch norm in:\n",
    "#\n",
    "\n",
    "n_embd = 10\n",
    "n_hidden = 100\n",
    "vocab_size = 27\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "# a viable variant is to put batch norm after the non-linearity here:\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), \n",
    "    Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), \n",
    "    Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), \n",
    "    Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), \n",
    "    Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), \n",
    "    Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size), \n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # make last layer less confident at initialization:\n",
    "    # layers[-1].weight *= 0.1\n",
    "    # Instead of weight for the last layer !!!\n",
    "    layers[-1].gamma *= 0.1\n",
    "    \n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 1.0\n",
    "            #layer.weight *= 5/3\n",
    "\n",
    "            \n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum([p.nelement() for p in parameters]))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "96630150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47024\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # last layer: make less confident\n",
    "    layers[-1].gamma *= 0.1\n",
    "    #layers[-1].weight *= 0.1\n",
    "    # all other layers: apply gain\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 1.0 #5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c510f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0,\tloss = 3.284299373626709\n",
      "step = 200,\tloss = 2.7293994426727295\n",
      "step = 400,\tloss = 2.7766332626342773\n",
      "step = 600,\tloss = 2.739665985107422\n",
      "step = 800,\tloss = 2.6246838569641113\n",
      "step = 1000,\tloss = 2.535635471343994\n",
      "step = 1200,\tloss = 2.358489990234375\n",
      "step = 1400,\tloss = 2.239806890487671\n",
      "step = 1600,\tloss = 2.61871337890625\n",
      "step = 1800,\tloss = 2.7463929653167725\n",
      "step = 2000,\tloss = 2.352499485015869\n",
      "step = 2200,\tloss = 1.9942113161087036\n",
      "step = 2400,\tloss = 2.388838529586792\n",
      "step = 2600,\tloss = 2.5457427501678467\n",
      "step = 2800,\tloss = 2.485955238342285\n",
      "step = 3000,\tloss = 2.556744337081909\n",
      "step = 3200,\tloss = 2.32403564453125\n",
      "step = 3400,\tloss = 2.346729278564453\n",
      "step = 3600,\tloss = 2.4430980682373047\n",
      "step = 3800,\tloss = 2.5304322242736816\n",
      "step = 4000,\tloss = 2.2273709774017334\n",
      "step = 4200,\tloss = 2.155994415283203\n",
      "step = 4400,\tloss = 2.457611083984375\n",
      "step = 4600,\tloss = 2.3537795543670654\n",
      "step = 4800,\tloss = 2.2429792881011963\n",
      "step = 5000,\tloss = 2.2977664470672607\n",
      "step = 5200,\tloss = 2.4629719257354736\n",
      "step = 5400,\tloss = 2.5571839809417725\n",
      "step = 5600,\tloss = 2.5106353759765625\n",
      "step = 5800,\tloss = 2.610100030899048\n"
     ]
    }
   ],
   "source": [
    "Steps = 200000\n",
    "batch_size=32\n",
    "lossi = []\n",
    "\n",
    "# update to data ratio:\n",
    "ud = []\n",
    "\n",
    "for step in range(Steps):\n",
    "\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # this is the forward pass\n",
    "    emb = C[Xb]\n",
    "    # emb.shape[0] is the batch size\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad() # only needed on the debug\n",
    "        \n",
    "    # intialize all gradients to 0\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # lr = 0.1 if step < Steps/2 else 0.01\n",
    "    lr = 0.1 if step < 150000 else 0.01 # step learning rate decay\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data -= lr*p.grad\n",
    "    \n",
    "    if step % int(Steps/1000) == 0:\n",
    "        print(f\"{step = },\\tloss = {loss.item()}\")\n",
    "  \n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([(lr*p.grad.std() / p.data.std()).log10().item() for p in parameters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec1d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
